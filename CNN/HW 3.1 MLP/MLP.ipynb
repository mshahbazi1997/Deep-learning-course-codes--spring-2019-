{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3. Part 1. MLP in Tensorflow (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline:   16 Farvardin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from utils import load_data\n",
    "from models import Dense\n",
    "from train import train\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# The following two lines let us reload external modules in the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description and Loading Data\n",
    "\n",
    "On this notebook, we are going to work on farsi OCR dataset. As its name implies, it is like famous **MNIST** dataset but it consists of images of handwritten digits in farsi. Each instance of this dataset is 32 * 32 gray-scale image. It is totally composed of 80000 instances. After loading this data, let's plot some images in order to see how they look like.\n",
    "\n",
    "Train, validation and test sets are loaded using a method in `utils.py`. Training set includes 0.7 of the whole dataset and test set just has 0.1 of it. Rest is assigned as validation set.\n",
    "\n",
    "**Note**: Images are flattened that's why their size is 1024 = 32 * 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the whole dataset...\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sample = 10\n",
    "num_class = 10\n",
    "fig, ax = plt.subplots(nrows=num_sample, ncols=num_class)\n",
    "\n",
    "for i in range(num_class):\n",
    "    class_i_images = [x_train[k] for k in range(x_train.shape[0]) if y_train[k][i] == 1]\n",
    "    for j in range(num_sample):\n",
    "        ax[j, i].get_xaxis().set_visible(False)\n",
    "        ax[j, i].get_yaxis().set_visible(False)\n",
    "        ax[j, i].imshow(class_i_images[j].reshape((32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Flags\n",
    "Tensorflow has the option of setting of some flags which should be defined once and can be used in any file later.\n",
    "\n",
    "Here we define flags for\n",
    "1. `learning_rate`: Shows the initial learning rate in optimization process.\n",
    "\n",
    "2. `num_epoch`: The total number of epochs for training process.\n",
    "\n",
    "3. `dropout`: The rate by which units are dropped in network.\n",
    "\n",
    "4. `weight_decay`: The coeffecient of L2 Loss term in total Loss function.\n",
    "\n",
    "5. `batch_size`: Size of each batch given to model.\n",
    "\n",
    "6. `early_stopping`: A great method in order to prevent overfitting is to check how loss function changes over validation set as training goes on. If within a window of a specific size the loss function shows an upward movement then it may be the sign of overfitting. Consequenltly, training process should be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epoch', 20, 'Number of epochs to train.')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0., 'Weight for L2 loss')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Size of batch')\n",
    "flags.DEFINE_integer('early_stopping', 5, 'Window size of early stopping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Initializers\n",
    "\n",
    "In the following cell we are importing some intializers which are defined in `utils.py`. Based on their explanation, you have to complete their code in `utils.py` and then import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glorot_initializer, zero_initializer, normal_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing MLP Model\n",
    "\n",
    "`Dense` model is defined as a python class in `models.py`. Its constructor receives as input followings:\n",
    "\n",
    "1. num_hidden: Number of hidden units and output units.\n",
    "2. weight_initializer: Function used for initializing weights\n",
    "3. bias_initializer: Function used for initializing biases\n",
    "4. act: Activation function used for hidden layers\n",
    "5. logging: This is a boolean showing whether the model saves log of weights and biases for later visualization using tensorboard.\n",
    "6. stddev: Standard deviation in case of having normal initializer for weights of layers.\n",
    "\n",
    "You have to complete some parts of `__init__`, `_loss`, `_accuracy`, `_log_vars` and `_build` methods in this class.\n",
    "\n",
    "There is also another python file `layers.py` consists of the class `DenseLayer`. The arguments of its constructor are:\n",
    "1. input_dim: Dimension of input to layer\n",
    "2. output_dim: Dimension of output of layer\n",
    "3. act: Activation function of layer\n",
    "4. weight_initializer: Function used for initializing weights\n",
    "5. bias_initializer: Function used for initializing biases\n",
    "6. stddev: Standard deviation in case of having normal initializer for weights of the layer.\n",
    "\n",
    "Some part of `__call__` method of is left for you to complete.\n",
    "\n",
    "**Note**: It is necessary to complete aforementioned python files before moving forward to following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting on features of an MLP\n",
    "\n",
    "In the rest of this notebook, some experiments should be done on different setting like regularization, activation function, number of layers, etc combined with some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the following cell you have to define a model with number of hidden units 200, 100, 50, 10. Note that the outputs of last 10 units, after which a softmax function is applied, act as scores for 10 class of digits in data.\n",
    "\n",
    "You also have to use uniform glorot initializer and zero initializer for weights and biases, respectively.\n",
    "\n",
    "The last point here is that we are using **sigmoid** as activation function of all layers.\n",
    "\n",
    "Later, we will apply another one and observe its difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(100,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 100) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(100, 50) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 100), dtype=float32)\n",
      "4\n",
      "<tf.Variable 'Variable_6:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_7:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_2:0\", shape=(?, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_hidden = [200, 100, 50, 10]\n",
    "model = Dense(num_hidden=[200, 100, 50, 10],\n",
    "              weight_initializer=glorot_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.sigmoid,\n",
    "              logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model we need to trigger training process by using the code partly prepared in `train.py`. After filling required lines run the following cell to start training.\n",
    "In this file some writers are defined which are later used for plotting visualizations in tensorboard framework. Summary information defined as scalars (like loss) and histograms (like weights) are saved by this writers in `logs` folder near existing files. More specifically, for each model another folder whose name came from `log_file` variable is created.\n",
    "\n",
    "Furthermore, when you define a session using `with`, the session is just restricted to its following context and can not be used in outer scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=2.315, train acc=0.100\n",
      "\n",
      "Epoch 2: train loss=2.301, train acc=0.138\n",
      "\n",
      "Epoch 3: train loss=2.298, train acc=0.176\n",
      "\n",
      "Epoch 4: train loss=2.297, train acc=0.181\n",
      "\n",
      "Epoch 5: train loss=2.296, train acc=0.194\n",
      "\n",
      "Epoch 6: train loss=2.294, train acc=0.211\n",
      "\n",
      "Epoch 7: train loss=2.293, train acc=0.234\n",
      "\n",
      "Epoch 8: train loss=2.292, train acc=0.257\n",
      "\n",
      "Epoch 9: train loss=2.291, train acc=0.282\n",
      "\n",
      "Epoch 10: train loss=2.290, train acc=0.305\n",
      "\n",
      "Epoch 11: train loss=2.289, train acc=0.330\n",
      "\n",
      "Epoch 12: train loss=2.288, train acc=0.355\n",
      "\n",
      "Epoch 13: train loss=2.286, train acc=0.379\n",
      "\n",
      "Epoch 14: train loss=2.285, train acc=0.402\n",
      "\n",
      "Epoch 15: train loss=2.284, train acc=0.425\n",
      "\n",
      "Epoch 16: train loss=2.283, train acc=0.445\n",
      "\n",
      "Epoch 17: train loss=2.281, train acc=0.465\n",
      "\n",
      "Epoch 18: train loss=2.280, train acc=0.484\n",
      "\n",
      "Epoch 19: train loss=2.279, train acc=0.501\n",
      "\n",
      "Epoch 20: train loss=2.277, train acc=0.519\n",
      "\n",
      "Test: average loss=2.278, average accuracy=0.513\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** In order to run different computation graphs in the same process, it is compulsary to reset the default graph to a new one which is defined later. To do so, you have to use method `tf.rest_default_graph()` as in the following cell. Otherwise, you will end up with lots of nodes in the default graph of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's maintain the same settings but change activation function to **tanh** and run the whole process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(100,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 100) dtype=float32_ref> Tensor(\"Tanh:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(100, 50) dtype=float32_ref> Tensor(\"Tanh_1:0\", shape=(?, 100), dtype=float32)\n",
      "4\n",
      "<tf.Variable 'Variable_6:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_7:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Tanh_2:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=1.174, train acc=0.778\n",
      "\n",
      "Epoch 2: train loss=0.828, train acc=0.834\n",
      "\n",
      "Epoch 3: train loss=0.667, train acc=0.858\n",
      "\n",
      "Epoch 4: train loss=0.571, train acc=0.872\n",
      "\n",
      "Epoch 5: train loss=0.507, train acc=0.883\n",
      "\n",
      "Epoch 6: train loss=0.460, train acc=0.891\n",
      "\n",
      "Epoch 7: train loss=0.423, train acc=0.897\n",
      "\n",
      "Epoch 8: train loss=0.394, train acc=0.903\n",
      "\n",
      "Epoch 9: train loss=0.371, train acc=0.908\n",
      "\n",
      "Epoch 10: train loss=0.351, train acc=0.912\n",
      "\n",
      "Epoch 11: train loss=0.334, train acc=0.916\n",
      "\n",
      "Epoch 12: train loss=0.320, train acc=0.919\n",
      "\n",
      "Epoch 13: train loss=0.307, train acc=0.922\n",
      "\n",
      "Epoch 14: train loss=0.296, train acc=0.924\n",
      "\n",
      "Epoch 15: train loss=0.286, train acc=0.926\n",
      "\n",
      "Epoch 16: train loss=0.278, train acc=0.928\n",
      "\n",
      "Epoch 17: train loss=0.270, train acc=0.930\n",
      "\n",
      "Epoch 18: train loss=0.262, train acc=0.932\n",
      "\n",
      "Epoch 19: train loss=0.256, train acc=0.933\n",
      "\n",
      "Epoch 20: train loss=0.250, train acc=0.935\n",
      "\n",
      "Test: average loss=0.255, average accuracy=0.934\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "model = Dense(num_hidden=[200, 100, 50, 10],\n",
    "              weight_initializer=glorot_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.tanh,\n",
    "              logging=True)\n",
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 1\n",
    "Compare `tanh` and `sigmoid` based on above results. Explain your observation from the visualizations produced by tensorboard.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Initializations\n",
    "\n",
    "Next we want to see the effect of aforementioned initializers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we use zero initializer both for biases and weights. By doing so, specially for weight initialization, the network will get in trouble with breaking the symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 50) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 2: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 3: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 4: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 5: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 6: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 7: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 8: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 9: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 10: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 11: train loss=2.303, train acc=0.101\n",
      "\n",
      "Early stopping on epoch 11...\n",
      "Test: average loss=2.303, average accuracy=0.098\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "model = Dense(num_hidden=[200, 50, 10],\n",
    "              weight_initializer=zero_initializer,\n",
    "              bias_initializer=zero_initializer,\n",
    "              act=tf.nn.sigmoid,\n",
    "              logging=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    log_file = str(model.act.__name__) + \"_\" + str(model.weight_initializer.__name__)\n",
    "    train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "tf.reset_default_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Question 2\n",
    "\n",
    "Use tensorboard visualizations of weights and learning curves (like loss and accuracy) as well to discuss about the issue raised by using zeros initializer for weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the impact of using normal initializer for weights.\n",
    "\n",
    "Do not forget to send in standard deviation of gaussain distribution used for sampling weights as an argument to model constructor for all next experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Initializer with stddev: 10\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 50) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=71.950, train acc=0.097\n",
      "\n",
      "Epoch 2: train loss=64.707, train acc=0.098\n",
      "\n",
      "Epoch 3: train loss=59.131, train acc=0.099\n",
      "\n",
      "Epoch 4: train loss=54.844, train acc=0.101\n",
      "\n",
      "Epoch 5: train loss=51.610, train acc=0.103\n",
      "\n",
      "Epoch 6: train loss=49.099, train acc=0.105\n",
      "\n",
      "Epoch 7: train loss=47.072, train acc=0.107\n",
      "\n",
      "Epoch 8: train loss=45.380, train acc=0.108\n",
      "\n",
      "Epoch 9: train loss=43.956, train acc=0.110\n",
      "\n",
      "Epoch 10: train loss=42.752, train acc=0.112\n",
      "\n",
      "Epoch 11: train loss=41.705, train acc=0.113\n",
      "\n",
      "Epoch 12: train loss=40.782, train acc=0.115\n",
      "\n",
      "Epoch 13: train loss=39.967, train acc=0.116\n",
      "\n",
      "Epoch 14: train loss=39.235, train acc=0.117\n",
      "\n",
      "Epoch 15: train loss=38.564, train acc=0.118\n",
      "\n",
      "Epoch 16: train loss=37.943, train acc=0.119\n",
      "\n",
      "Epoch 17: train loss=37.367, train acc=0.120\n",
      "\n",
      "Epoch 18: train loss=36.831, train acc=0.122\n",
      "\n",
      "Epoch 19: train loss=36.326, train acc=0.122\n",
      "\n",
      "Epoch 20: train loss=35.847, train acc=0.123\n",
      "\n",
      "Test: average loss=35.785, average accuracy=0.123\n",
      "-------\n",
      "Normal Initializer with stddev: 1\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 50) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=4.990, train acc=0.110\n",
      "\n",
      "Epoch 2: train loss=3.891, train acc=0.120\n",
      "\n",
      "Epoch 3: train loss=3.334, train acc=0.141\n",
      "\n",
      "Epoch 4: train loss=2.997, train acc=0.167\n",
      "\n",
      "Epoch 5: train loss=2.780, train acc=0.192\n",
      "\n",
      "Epoch 6: train loss=2.635, train acc=0.216\n",
      "\n",
      "Epoch 7: train loss=2.519, train acc=0.237\n",
      "\n",
      "Epoch 8: train loss=2.417, train acc=0.259\n",
      "\n",
      "Epoch 9: train loss=2.326, train acc=0.280\n",
      "\n",
      "Epoch 10: train loss=2.243, train acc=0.300\n",
      "\n",
      "Epoch 11: train loss=2.168, train acc=0.319\n",
      "\n",
      "Epoch 12: train loss=2.100, train acc=0.336\n",
      "\n",
      "Epoch 13: train loss=2.038, train acc=0.353\n",
      "\n",
      "Epoch 14: train loss=1.981, train acc=0.367\n",
      "\n",
      "Epoch 15: train loss=1.929, train acc=0.382\n",
      "\n",
      "Epoch 16: train loss=1.880, train acc=0.395\n",
      "\n",
      "Epoch 17: train loss=1.835, train acc=0.408\n",
      "\n",
      "Epoch 18: train loss=1.793, train acc=0.420\n",
      "\n",
      "Epoch 19: train loss=1.754, train acc=0.433\n",
      "\n",
      "Epoch 20: train loss=1.717, train acc=0.444\n",
      "\n",
      "Test: average loss=1.697, average accuracy=0.457\n",
      "-------\n",
      "Normal Initializer with stddev: 0.01\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 200) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(200, 50) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 200), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=2.303, train acc=0.100\n",
      "\n",
      "Epoch 2: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 3: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 4: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 5: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 6: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 7: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 8: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 9: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 10: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 11: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 12: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 13: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 14: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 15: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 16: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 17: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 18: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 19: train loss=2.303, train acc=0.101\n",
      "\n",
      "Epoch 20: train loss=2.303, train acc=0.101\n",
      "\n",
      "Test: average loss=2.303, average accuracy=0.098\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "stddev_list = [10, 1, 0.01]\n",
    "for stddev in stddev_list:\n",
    "    print('Normal Initializer with stddev: {}'.format(stddev))\n",
    "    model = Dense(num_hidden=[200, 50, 10],\n",
    "                  weight_initializer=normal_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.sigmoid,\n",
    "                  logging=True,\n",
    "                  stddev=stddev)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(model.weight_initializer.__name__) + \"_\" + str(stddev)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 3\n",
    "\n",
    "Use tensorboard visualizations of weights and learning curves (like loss and accuracy) as well to describe differences through training process caused by using different standard deviations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Architectures (layers and units)\n",
    "\n",
    "Next, we will cast light on the importance of architecture of neural networks (more specifically number of layers and units)\n",
    "\n",
    "In the next 2 experiments, there is no need to have that much number of epochs like before, so will set to 50. We also use normal initializer for weights with `stddev=1` and `sigmoid` as activation function. The only change is in the number of layers and units.\n",
    "\n",
    "In the first one we have just one hidden layer comprising 500 units which imposes great computational cost. On the other hand, the second network has one more hidden layer with less number of hidden units. Totally it has less parameters compared with the first network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden units:  [500, 10]\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(500,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 500) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(500, 10) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 500), dtype=float32)\n",
      "Epoch 1: train loss=11.563, train acc=0.097\n",
      "\n",
      "Epoch 2: train loss=9.283, train acc=0.156\n",
      "\n",
      "Epoch 3: train loss=7.617, train acc=0.229\n",
      "\n",
      "Epoch 4: train loss=6.418, train acc=0.298\n",
      "\n",
      "Epoch 5: train loss=5.552, train acc=0.360\n",
      "\n",
      "Epoch 6: train loss=4.910, train acc=0.411\n",
      "\n",
      "Epoch 7: train loss=4.421, train acc=0.454\n",
      "\n",
      "Epoch 8: train loss=4.038, train acc=0.490\n",
      "\n",
      "Epoch 9: train loss=3.732, train acc=0.520\n",
      "\n",
      "Epoch 10: train loss=3.482, train acc=0.546\n",
      "\n",
      "Epoch 11: train loss=3.274, train acc=0.568\n",
      "\n",
      "Epoch 12: train loss=3.099, train acc=0.588\n",
      "\n",
      "Epoch 13: train loss=2.949, train acc=0.605\n",
      "\n",
      "Epoch 14: train loss=2.818, train acc=0.619\n",
      "\n",
      "Epoch 15: train loss=2.703, train acc=0.632\n",
      "\n",
      "Epoch 16: train loss=2.601, train acc=0.644\n",
      "\n",
      "Epoch 17: train loss=2.510, train acc=0.655\n",
      "\n",
      "Epoch 18: train loss=2.427, train acc=0.664\n",
      "\n",
      "Epoch 19: train loss=2.352, train acc=0.672\n",
      "\n",
      "Epoch 20: train loss=2.283, train acc=0.680\n",
      "\n",
      "Epoch 21: train loss=2.220, train acc=0.688\n",
      "\n",
      "Epoch 22: train loss=2.162, train acc=0.695\n",
      "\n",
      "Epoch 23: train loss=2.108, train acc=0.702\n",
      "\n",
      "Epoch 24: train loss=2.058, train acc=0.708\n",
      "\n",
      "Epoch 25: train loss=2.011, train acc=0.714\n",
      "\n",
      "Epoch 26: train loss=1.967, train acc=0.719\n",
      "\n",
      "Epoch 27: train loss=1.926, train acc=0.725\n",
      "\n",
      "Epoch 28: train loss=1.887, train acc=0.729\n",
      "\n",
      "Epoch 29: train loss=1.850, train acc=0.733\n",
      "\n",
      "Epoch 30: train loss=1.815, train acc=0.738\n",
      "\n",
      "Epoch 31: train loss=1.782, train acc=0.742\n",
      "\n",
      "Epoch 32: train loss=1.751, train acc=0.746\n",
      "\n",
      "Epoch 33: train loss=1.721, train acc=0.749\n",
      "\n",
      "Epoch 34: train loss=1.693, train acc=0.753\n",
      "\n",
      "Epoch 35: train loss=1.665, train acc=0.756\n",
      "\n",
      "Epoch 36: train loss=1.639, train acc=0.759\n",
      "\n",
      "Epoch 37: train loss=1.615, train acc=0.762\n",
      "\n",
      "Epoch 38: train loss=1.591, train acc=0.765\n",
      "\n",
      "Epoch 39: train loss=1.568, train acc=0.768\n",
      "\n",
      "Epoch 40: train loss=1.546, train acc=0.771\n",
      "\n",
      "Epoch 41: train loss=1.525, train acc=0.774\n",
      "\n",
      "Epoch 42: train loss=1.505, train acc=0.776\n",
      "\n",
      "Epoch 43: train loss=1.485, train acc=0.779\n",
      "\n",
      "Epoch 44: train loss=1.466, train acc=0.781\n",
      "\n",
      "Epoch 45: train loss=1.448, train acc=0.784\n",
      "\n",
      "Epoch 46: train loss=1.431, train acc=0.786\n",
      "\n",
      "Epoch 47: train loss=1.414, train acc=0.788\n",
      "\n",
      "Epoch 48: train loss=1.397, train acc=0.790\n",
      "\n",
      "Epoch 49: train loss=1.381, train acc=0.792\n",
      "\n",
      "Epoch 50: train loss=1.366, train acc=0.794\n",
      "\n",
      "Test: average loss=1.430, average accuracy=0.793\n",
      "-------\n",
      "Number of hidden units:  [100, 50, 10]\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(100,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 100) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(50,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(100, 50) dtype=float32_ref> Tensor(\"Sigmoid:0\", shape=(?, 100), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(50, 10) dtype=float32_ref> Tensor(\"Sigmoid_1:0\", shape=(?, 50), dtype=float32)\n",
      "Epoch 1: train loss=4.592, train acc=0.101\n",
      "\n",
      "Epoch 2: train loss=3.795, train acc=0.115\n",
      "\n",
      "Epoch 3: train loss=3.261, train acc=0.131\n",
      "\n",
      "Epoch 4: train loss=2.946, train acc=0.148\n",
      "\n",
      "Epoch 5: train loss=2.758, train acc=0.168\n",
      "\n",
      "Epoch 6: train loss=2.624, train acc=0.190\n",
      "\n",
      "Epoch 7: train loss=2.513, train acc=0.211\n",
      "\n",
      "Epoch 8: train loss=2.415, train acc=0.233\n",
      "\n",
      "Epoch 9: train loss=2.326, train acc=0.253\n",
      "\n",
      "Epoch 10: train loss=2.246, train acc=0.273\n",
      "\n",
      "Epoch 11: train loss=2.173, train acc=0.293\n",
      "\n",
      "Epoch 12: train loss=2.106, train acc=0.310\n",
      "\n",
      "Epoch 13: train loss=2.045, train acc=0.328\n",
      "\n",
      "Epoch 14: train loss=1.988, train acc=0.345\n",
      "\n",
      "Epoch 15: train loss=1.936, train acc=0.360\n",
      "\n",
      "Epoch 16: train loss=1.887, train acc=0.376\n",
      "\n",
      "Epoch 17: train loss=1.841, train acc=0.391\n",
      "\n",
      "Epoch 18: train loss=1.799, train acc=0.405\n",
      "\n",
      "Epoch 19: train loss=1.759, train acc=0.418\n",
      "\n",
      "Epoch 20: train loss=1.721, train acc=0.431\n",
      "\n",
      "Epoch 21: train loss=1.686, train acc=0.443\n",
      "\n",
      "Epoch 22: train loss=1.652, train acc=0.454\n",
      "\n",
      "Epoch 23: train loss=1.620, train acc=0.465\n",
      "\n",
      "Epoch 24: train loss=1.590, train acc=0.475\n",
      "\n",
      "Epoch 25: train loss=1.561, train acc=0.485\n",
      "\n",
      "Epoch 26: train loss=1.534, train acc=0.495\n",
      "\n",
      "Epoch 27: train loss=1.508, train acc=0.504\n",
      "\n",
      "Epoch 28: train loss=1.483, train acc=0.513\n",
      "\n",
      "Epoch 29: train loss=1.459, train acc=0.522\n",
      "\n",
      "Epoch 30: train loss=1.437, train acc=0.529\n",
      "\n",
      "Epoch 31: train loss=1.415, train acc=0.537\n",
      "\n",
      "Epoch 32: train loss=1.394, train acc=0.545\n",
      "\n",
      "Epoch 33: train loss=1.374, train acc=0.551\n",
      "\n",
      "Epoch 34: train loss=1.355, train acc=0.559\n",
      "\n",
      "Epoch 35: train loss=1.336, train acc=0.566\n",
      "\n",
      "Epoch 36: train loss=1.318, train acc=0.572\n",
      "\n",
      "Epoch 37: train loss=1.301, train acc=0.578\n",
      "\n",
      "Epoch 38: train loss=1.284, train acc=0.584\n",
      "\n",
      "Epoch 39: train loss=1.268, train acc=0.590\n",
      "\n",
      "Epoch 40: train loss=1.253, train acc=0.595\n",
      "\n",
      "Epoch 41: train loss=1.238, train acc=0.600\n",
      "\n",
      "Epoch 42: train loss=1.223, train acc=0.606\n",
      "\n",
      "Epoch 43: train loss=1.209, train acc=0.610\n",
      "\n",
      "Epoch 44: train loss=1.196, train acc=0.615\n",
      "\n",
      "Epoch 45: train loss=1.183, train acc=0.620\n",
      "\n",
      "Epoch 46: train loss=1.170, train acc=0.625\n",
      "\n",
      "Epoch 47: train loss=1.158, train acc=0.629\n",
      "\n",
      "Epoch 48: train loss=1.146, train acc=0.633\n",
      "\n",
      "Epoch 49: train loss=1.134, train acc=0.637\n",
      "\n",
      "Epoch 50: train loss=1.123, train acc=0.641\n",
      "\n",
      "Test: average loss=1.133, average accuracy=0.637\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "FLAGS.num_epoch = 50\n",
    "num_hidden_list = [[500, 10], [100, 50, 10]]\n",
    "stddev = 1\n",
    "for num_hidden in num_hidden_list:\n",
    "    print('Number of hidden units: ', num_hidden)\n",
    "    model = Dense(num_hidden=num_hidden,\n",
    "                  weight_initializer=normal_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.sigmoid,\n",
    "                  logging=True,\n",
    "                  stddev=stddev)\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(model.num_hidden)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 4\n",
    "By using tensorboard visualiztions justify the different outcomes of training the above two neural network architectures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Another important point to consider is applying regularization and being aware of its impact on training and generalization. It could be helpful technique in many cases you can not progress in training.\n",
    "\n",
    "About the setting, we change the learing rate to 0.0001 and number of epochs back to 100.\n",
    "\n",
    "In first experiment L2 regularization technique is not applied. However, in rest of them, it is used for penalizing two sets of weights:\n",
    "\n",
    "1. Weights between input and first hidden layer.\n",
    "2. Weights between first and second hidden layer.\n",
    "\n",
    "Another point to mention is that here we use `relu` activation function for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay of: 0.0\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(256,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 256) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(128,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(256, 128) dtype=float32_ref> Tensor(\"Relu:0\", shape=(?, 256), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(128, 10) dtype=float32_ref> Tensor(\"Relu_1:0\", shape=(?, 128), dtype=float32)\n",
      "Epoch 1: train loss=2.243, train acc=0.205\n",
      "\n",
      "Epoch 2: train loss=2.149, train acc=0.277\n",
      "\n",
      "Epoch 3: train loss=2.062, train acc=0.359\n",
      "\n",
      "Epoch 4: train loss=1.980, train acc=0.446\n",
      "\n",
      "Epoch 5: train loss=1.900, train acc=0.520\n",
      "\n",
      "Epoch 6: train loss=1.822, train acc=0.579\n",
      "\n",
      "Epoch 7: train loss=1.745, train acc=0.628\n",
      "\n",
      "Epoch 8: train loss=1.668, train acc=0.667\n",
      "\n",
      "Epoch 9: train loss=1.593, train acc=0.699\n",
      "\n",
      "Epoch 10: train loss=1.519, train acc=0.724\n",
      "\n",
      "Epoch 11: train loss=1.447, train acc=0.745\n",
      "\n",
      "Epoch 12: train loss=1.378, train acc=0.762\n",
      "\n",
      "Epoch 13: train loss=1.311, train acc=0.776\n",
      "\n",
      "Epoch 14: train loss=1.248, train acc=0.787\n",
      "\n",
      "Epoch 15: train loss=1.188, train acc=0.797\n",
      "\n",
      "Epoch 16: train loss=1.132, train acc=0.804\n",
      "\n",
      "Epoch 17: train loss=1.079, train acc=0.811\n",
      "\n",
      "Epoch 18: train loss=1.031, train acc=0.817\n",
      "\n",
      "Epoch 19: train loss=0.985, train acc=0.823\n",
      "\n",
      "Epoch 20: train loss=0.943, train acc=0.827\n",
      "\n",
      "Epoch 21: train loss=0.904, train acc=0.831\n",
      "\n",
      "Epoch 22: train loss=0.868, train acc=0.835\n",
      "\n",
      "Epoch 23: train loss=0.835, train acc=0.838\n",
      "\n",
      "Epoch 24: train loss=0.804, train acc=0.841\n",
      "\n",
      "Epoch 25: train loss=0.776, train acc=0.845\n",
      "\n",
      "Epoch 26: train loss=0.750, train acc=0.847\n",
      "\n",
      "Epoch 27: train loss=0.726, train acc=0.850\n",
      "\n",
      "Epoch 28: train loss=0.704, train acc=0.852\n",
      "\n",
      "Epoch 29: train loss=0.683, train acc=0.855\n",
      "\n",
      "Epoch 30: train loss=0.664, train acc=0.857\n",
      "\n",
      "Epoch 31: train loss=0.646, train acc=0.859\n",
      "\n",
      "Epoch 32: train loss=0.630, train acc=0.861\n",
      "\n",
      "Epoch 33: train loss=0.614, train acc=0.863\n",
      "\n",
      "Epoch 34: train loss=0.600, train acc=0.865\n",
      "\n",
      "Epoch 35: train loss=0.586, train acc=0.867\n",
      "\n",
      "Epoch 36: train loss=0.573, train acc=0.869\n",
      "\n",
      "Epoch 37: train loss=0.561, train acc=0.870\n",
      "\n",
      "Epoch 38: train loss=0.550, train acc=0.872\n",
      "\n",
      "Epoch 39: train loss=0.539, train acc=0.873\n",
      "\n",
      "Epoch 40: train loss=0.529, train acc=0.874\n",
      "\n",
      "Epoch 41: train loss=0.520, train acc=0.875\n",
      "\n",
      "Epoch 42: train loss=0.511, train acc=0.876\n",
      "\n",
      "Epoch 43: train loss=0.502, train acc=0.877\n",
      "\n",
      "Epoch 44: train loss=0.494, train acc=0.879\n",
      "\n",
      "Epoch 45: train loss=0.486, train acc=0.880\n",
      "\n",
      "Epoch 46: train loss=0.479, train acc=0.881\n",
      "\n",
      "Epoch 47: train loss=0.472, train acc=0.882\n",
      "\n",
      "Epoch 48: train loss=0.465, train acc=0.883\n",
      "\n",
      "Epoch 49: train loss=0.458, train acc=0.884\n",
      "\n",
      "Epoch 50: train loss=0.452, train acc=0.885\n",
      "\n",
      "Epoch 51: train loss=0.446, train acc=0.886\n",
      "\n",
      "Epoch 52: train loss=0.441, train acc=0.887\n",
      "\n",
      "Epoch 53: train loss=0.435, train acc=0.888\n",
      "\n",
      "Epoch 54: train loss=0.430, train acc=0.889\n",
      "\n",
      "Epoch 55: train loss=0.425, train acc=0.890\n",
      "\n",
      "Epoch 56: train loss=0.420, train acc=0.891\n",
      "\n",
      "Epoch 57: train loss=0.415, train acc=0.891\n",
      "\n",
      "Epoch 58: train loss=0.411, train acc=0.892\n",
      "\n",
      "Epoch 59: train loss=0.406, train acc=0.893\n",
      "\n",
      "Epoch 60: train loss=0.402, train acc=0.894\n",
      "\n",
      "Epoch 61: train loss=0.398, train acc=0.895\n",
      "\n",
      "Epoch 62: train loss=0.394, train acc=0.895\n",
      "\n",
      "Epoch 63: train loss=0.390, train acc=0.896\n",
      "\n",
      "Epoch 64: train loss=0.387, train acc=0.897\n",
      "\n",
      "Epoch 65: train loss=0.383, train acc=0.898\n",
      "\n",
      "Epoch 66: train loss=0.380, train acc=0.898\n",
      "\n",
      "Epoch 67: train loss=0.376, train acc=0.899\n",
      "\n",
      "Epoch 68: train loss=0.373, train acc=0.900\n",
      "\n",
      "Epoch 69: train loss=0.370, train acc=0.901\n",
      "\n",
      "Epoch 70: train loss=0.367, train acc=0.901\n",
      "\n",
      "Epoch 71: train loss=0.364, train acc=0.902\n",
      "\n",
      "Epoch 72: train loss=0.361, train acc=0.902\n",
      "\n",
      "Epoch 73: train loss=0.358, train acc=0.903\n",
      "\n",
      "Epoch 74: train loss=0.355, train acc=0.904\n",
      "\n",
      "Epoch 75: train loss=0.353, train acc=0.905\n",
      "\n",
      "Epoch 76: train loss=0.350, train acc=0.905\n",
      "\n",
      "Epoch 77: train loss=0.347, train acc=0.906\n",
      "\n",
      "Epoch 78: train loss=0.345, train acc=0.906\n",
      "\n",
      "Epoch 79: train loss=0.342, train acc=0.907\n",
      "\n",
      "Epoch 80: train loss=0.340, train acc=0.907\n",
      "\n",
      "Epoch 81: train loss=0.338, train acc=0.908\n",
      "\n",
      "Epoch 82: train loss=0.335, train acc=0.908\n",
      "\n",
      "Epoch 83: train loss=0.333, train acc=0.909\n",
      "\n",
      "Epoch 84: train loss=0.331, train acc=0.909\n",
      "\n",
      "Epoch 85: train loss=0.329, train acc=0.909\n",
      "\n",
      "Epoch 86: train loss=0.327, train acc=0.910\n",
      "\n",
      "Epoch 87: train loss=0.325, train acc=0.911\n",
      "\n",
      "Epoch 88: train loss=0.323, train acc=0.911\n",
      "\n",
      "Epoch 89: train loss=0.321, train acc=0.911\n",
      "\n",
      "Epoch 90: train loss=0.319, train acc=0.912\n",
      "\n",
      "Epoch 91: train loss=0.317, train acc=0.912\n",
      "\n",
      "Epoch 92: train loss=0.315, train acc=0.913\n",
      "\n",
      "Epoch 93: train loss=0.314, train acc=0.913\n",
      "\n",
      "Epoch 94: train loss=0.312, train acc=0.914\n",
      "\n",
      "Epoch 95: train loss=0.310, train acc=0.914\n",
      "\n",
      "Epoch 96: train loss=0.309, train acc=0.915\n",
      "\n",
      "Epoch 97: train loss=0.307, train acc=0.915\n",
      "\n",
      "Epoch 98: train loss=0.305, train acc=0.916\n",
      "\n",
      "Epoch 99: train loss=0.304, train acc=0.916\n",
      "\n",
      "Epoch 100: train loss=0.302, train acc=0.917\n",
      "\n",
      "Test: average loss=0.304, average accuracy=0.914\n",
      "-------\n",
      "Weight decay of: 0.0001\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(256,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 256) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(128,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(256, 128) dtype=float32_ref> Tensor(\"Relu:0\", shape=(?, 256), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(128, 10) dtype=float32_ref> Tensor(\"Relu_1:0\", shape=(?, 128), dtype=float32)\n",
      "Epoch 1: train loss=2.346, train acc=0.117\n",
      "\n",
      "Epoch 2: train loss=2.260, train acc=0.171\n",
      "\n",
      "Epoch 3: train loss=2.181, train acc=0.246\n",
      "\n",
      "Epoch 4: train loss=2.105, train acc=0.333\n",
      "\n",
      "Epoch 5: train loss=2.031, train acc=0.421\n",
      "\n",
      "Epoch 6: train loss=1.958, train acc=0.501\n",
      "\n",
      "Epoch 7: train loss=1.884, train acc=0.571\n",
      "\n",
      "Epoch 8: train loss=1.810, train acc=0.627\n",
      "\n",
      "Epoch 9: train loss=1.736, train acc=0.672\n",
      "\n",
      "Epoch 10: train loss=1.663, train acc=0.708\n",
      "\n",
      "Epoch 11: train loss=1.589, train acc=0.734\n",
      "\n",
      "Epoch 12: train loss=1.517, train acc=0.755\n",
      "\n",
      "Epoch 13: train loss=1.447, train acc=0.772\n",
      "\n",
      "Epoch 14: train loss=1.378, train acc=0.785\n",
      "\n",
      "Epoch 15: train loss=1.312, train acc=0.795\n",
      "\n",
      "Epoch 16: train loss=1.250, train acc=0.804\n",
      "\n",
      "Epoch 17: train loss=1.190, train acc=0.812\n",
      "\n",
      "Epoch 18: train loss=1.134, train acc=0.818\n",
      "\n",
      "Epoch 19: train loss=1.082, train acc=0.824\n",
      "\n",
      "Epoch 20: train loss=1.034, train acc=0.830\n",
      "\n",
      "Epoch 21: train loss=0.989, train acc=0.834\n",
      "\n",
      "Epoch 22: train loss=0.948, train acc=0.838\n",
      "\n",
      "Epoch 23: train loss=0.910, train acc=0.842\n",
      "\n",
      "Epoch 24: train loss=0.875, train acc=0.845\n",
      "\n",
      "Epoch 25: train loss=0.843, train acc=0.848\n",
      "\n",
      "Epoch 26: train loss=0.813, train acc=0.850\n",
      "\n",
      "Epoch 27: train loss=0.786, train acc=0.852\n",
      "\n",
      "Epoch 28: train loss=0.761, train acc=0.854\n",
      "\n",
      "Epoch 29: train loss=0.738, train acc=0.856\n",
      "\n",
      "Epoch 30: train loss=0.716, train acc=0.858\n",
      "\n",
      "Epoch 31: train loss=0.697, train acc=0.860\n",
      "\n",
      "Epoch 32: train loss=0.678, train acc=0.861\n",
      "\n",
      "Epoch 33: train loss=0.661, train acc=0.863\n",
      "\n",
      "Epoch 34: train loss=0.646, train acc=0.864\n",
      "\n",
      "Epoch 35: train loss=0.631, train acc=0.866\n",
      "\n",
      "Epoch 36: train loss=0.617, train acc=0.868\n",
      "\n",
      "Epoch 37: train loss=0.604, train acc=0.869\n",
      "\n",
      "Epoch 38: train loss=0.592, train acc=0.871\n",
      "\n",
      "Epoch 39: train loss=0.581, train acc=0.872\n",
      "\n",
      "Epoch 40: train loss=0.570, train acc=0.873\n",
      "\n",
      "Epoch 41: train loss=0.560, train acc=0.874\n",
      "\n",
      "Epoch 42: train loss=0.551, train acc=0.875\n",
      "\n",
      "Epoch 43: train loss=0.542, train acc=0.877\n",
      "\n",
      "Epoch 44: train loss=0.533, train acc=0.878\n",
      "\n",
      "Epoch 45: train loss=0.525, train acc=0.878\n",
      "\n",
      "Epoch 46: train loss=0.517, train acc=0.880\n",
      "\n",
      "Epoch 47: train loss=0.510, train acc=0.881\n",
      "\n",
      "Epoch 48: train loss=0.503, train acc=0.882\n",
      "\n",
      "Epoch 49: train loss=0.496, train acc=0.882\n",
      "\n",
      "Epoch 50: train loss=0.490, train acc=0.883\n",
      "\n",
      "Epoch 51: train loss=0.484, train acc=0.884\n",
      "\n",
      "Epoch 52: train loss=0.478, train acc=0.885\n",
      "\n",
      "Epoch 53: train loss=0.473, train acc=0.886\n",
      "\n",
      "Epoch 54: train loss=0.467, train acc=0.887\n",
      "\n",
      "Epoch 55: train loss=0.462, train acc=0.888\n",
      "\n",
      "Epoch 56: train loss=0.457, train acc=0.889\n",
      "\n",
      "Epoch 57: train loss=0.452, train acc=0.889\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: train loss=0.448, train acc=0.890\n",
      "\n",
      "Epoch 59: train loss=0.443, train acc=0.891\n",
      "\n",
      "Epoch 60: train loss=0.439, train acc=0.892\n",
      "\n",
      "Epoch 61: train loss=0.435, train acc=0.892\n",
      "\n",
      "Epoch 62: train loss=0.431, train acc=0.893\n",
      "\n",
      "Epoch 63: train loss=0.427, train acc=0.894\n",
      "\n",
      "Epoch 64: train loss=0.423, train acc=0.895\n",
      "\n",
      "Epoch 65: train loss=0.420, train acc=0.895\n",
      "\n",
      "Epoch 66: train loss=0.416, train acc=0.896\n",
      "\n",
      "Epoch 67: train loss=0.413, train acc=0.897\n",
      "\n",
      "Epoch 68: train loss=0.410, train acc=0.897\n",
      "\n",
      "Epoch 69: train loss=0.406, train acc=0.898\n",
      "\n",
      "Epoch 70: train loss=0.403, train acc=0.899\n",
      "\n",
      "Epoch 71: train loss=0.400, train acc=0.900\n",
      "\n",
      "Epoch 72: train loss=0.397, train acc=0.900\n",
      "\n",
      "Epoch 73: train loss=0.395, train acc=0.901\n",
      "\n",
      "Epoch 74: train loss=0.392, train acc=0.901\n",
      "\n",
      "Epoch 75: train loss=0.389, train acc=0.902\n",
      "\n",
      "Epoch 76: train loss=0.386, train acc=0.903\n",
      "\n",
      "Epoch 77: train loss=0.384, train acc=0.903\n",
      "\n",
      "Epoch 78: train loss=0.381, train acc=0.904\n",
      "\n",
      "Epoch 79: train loss=0.379, train acc=0.905\n",
      "\n",
      "Epoch 80: train loss=0.377, train acc=0.905\n",
      "\n",
      "Epoch 81: train loss=0.374, train acc=0.906\n",
      "\n",
      "Epoch 82: train loss=0.372, train acc=0.906\n",
      "\n",
      "Epoch 83: train loss=0.370, train acc=0.907\n",
      "\n",
      "Epoch 84: train loss=0.368, train acc=0.907\n",
      "\n",
      "Epoch 85: train loss=0.365, train acc=0.908\n",
      "\n",
      "Epoch 86: train loss=0.363, train acc=0.908\n",
      "\n",
      "Epoch 87: train loss=0.361, train acc=0.909\n",
      "\n",
      "Epoch 88: train loss=0.359, train acc=0.909\n",
      "\n",
      "Epoch 89: train loss=0.357, train acc=0.910\n",
      "\n",
      "Epoch 90: train loss=0.356, train acc=0.910\n",
      "\n",
      "Epoch 91: train loss=0.354, train acc=0.911\n",
      "\n",
      "Epoch 92: train loss=0.352, train acc=0.911\n",
      "\n",
      "Epoch 93: train loss=0.350, train acc=0.911\n",
      "\n",
      "Epoch 94: train loss=0.348, train acc=0.912\n",
      "\n",
      "Epoch 95: train loss=0.347, train acc=0.912\n",
      "\n",
      "Epoch 96: train loss=0.345, train acc=0.913\n",
      "\n",
      "Epoch 97: train loss=0.343, train acc=0.913\n",
      "\n",
      "Epoch 98: train loss=0.342, train acc=0.913\n",
      "\n",
      "Epoch 99: train loss=0.340, train acc=0.913\n",
      "\n",
      "Epoch 100: train loss=0.338, train acc=0.914\n",
      "\n",
      "Test: average loss=0.338, average accuracy=0.913\n",
      "-------\n",
      "Weight decay of: 0.01\n",
      "1\n",
      "<tf.Variable 'Variable:0' shape=(256,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1024, 256) dtype=float32_ref> Tensor(\"Placeholder:0\", shape=(?, 1024), dtype=float32)\n",
      "2\n",
      "<tf.Variable 'Variable_2:0' shape=(128,) dtype=float32_ref> <tf.Variable 'Variable_3:0' shape=(256, 128) dtype=float32_ref> Tensor(\"Relu:0\", shape=(?, 256), dtype=float32)\n",
      "3\n",
      "<tf.Variable 'Variable_4:0' shape=(10,) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(128, 10) dtype=float32_ref> Tensor(\"Relu_1:0\", shape=(?, 128), dtype=float32)\n",
      "Epoch 1: train loss=5.156, train acc=0.219\n",
      "\n",
      "Epoch 2: train loss=5.041, train acc=0.314\n",
      "\n",
      "Epoch 3: train loss=4.937, train acc=0.414\n",
      "\n",
      "Epoch 4: train loss=4.840, train acc=0.502\n",
      "\n",
      "Epoch 5: train loss=4.747, train acc=0.571\n",
      "\n",
      "Epoch 6: train loss=4.656, train acc=0.622\n",
      "\n",
      "Epoch 7: train loss=4.567, train acc=0.662\n",
      "\n",
      "Epoch 8: train loss=4.480, train acc=0.696\n",
      "\n",
      "Epoch 9: train loss=4.396, train acc=0.725\n",
      "\n",
      "Epoch 10: train loss=4.314, train acc=0.749\n",
      "\n",
      "Epoch 11: train loss=4.236, train acc=0.768\n",
      "\n",
      "Epoch 12: train loss=4.161, train acc=0.784\n",
      "\n",
      "Epoch 13: train loss=4.090, train acc=0.797\n",
      "\n",
      "Epoch 14: train loss=4.023, train acc=0.808\n",
      "\n",
      "Epoch 15: train loss=3.961, train acc=0.817\n",
      "\n",
      "Epoch 16: train loss=3.903, train acc=0.824\n",
      "\n",
      "Epoch 17: train loss=3.849, train acc=0.829\n",
      "\n",
      "Epoch 18: train loss=3.800, train acc=0.835\n",
      "\n",
      "Epoch 19: train loss=3.755, train acc=0.839\n",
      "\n",
      "Epoch 20: train loss=3.713, train acc=0.842\n",
      "\n",
      "Epoch 21: train loss=3.675, train acc=0.845\n",
      "\n",
      "Epoch 22: train loss=3.640, train acc=0.848\n",
      "\n",
      "Epoch 23: train loss=3.607, train acc=0.851\n",
      "\n",
      "Epoch 24: train loss=3.578, train acc=0.853\n",
      "\n",
      "Epoch 25: train loss=3.550, train acc=0.855\n",
      "\n",
      "Epoch 26: train loss=3.525, train acc=0.857\n",
      "\n",
      "Epoch 27: train loss=3.501, train acc=0.859\n",
      "\n",
      "Epoch 28: train loss=3.479, train acc=0.860\n",
      "\n",
      "Epoch 29: train loss=3.458, train acc=0.862\n",
      "\n",
      "Epoch 30: train loss=3.439, train acc=0.864\n",
      "\n",
      "Epoch 31: train loss=3.421, train acc=0.865\n",
      "\n",
      "Epoch 32: train loss=3.404, train acc=0.867\n",
      "\n",
      "Epoch 33: train loss=3.388, train acc=0.868\n",
      "\n",
      "Epoch 34: train loss=3.373, train acc=0.870\n",
      "\n",
      "Epoch 35: train loss=3.359, train acc=0.871\n",
      "\n",
      "Epoch 36: train loss=3.346, train acc=0.872\n",
      "\n",
      "Epoch 37: train loss=3.333, train acc=0.873\n",
      "\n",
      "Epoch 38: train loss=3.320, train acc=0.875\n",
      "\n",
      "Epoch 39: train loss=3.309, train acc=0.876\n",
      "\n",
      "Epoch 40: train loss=3.298, train acc=0.877\n",
      "\n",
      "Epoch 41: train loss=3.287, train acc=0.878\n",
      "\n",
      "Epoch 42: train loss=3.276, train acc=0.879\n",
      "\n",
      "Epoch 43: train loss=3.266, train acc=0.880\n",
      "\n",
      "Epoch 44: train loss=3.257, train acc=0.881\n",
      "\n",
      "Epoch 45: train loss=3.248, train acc=0.882\n",
      "\n",
      "Epoch 46: train loss=3.239, train acc=0.883\n",
      "\n",
      "Epoch 47: train loss=3.230, train acc=0.884\n",
      "\n",
      "Epoch 48: train loss=3.222, train acc=0.885\n",
      "\n",
      "Epoch 49: train loss=3.213, train acc=0.886\n",
      "\n",
      "Epoch 50: train loss=3.205, train acc=0.887\n",
      "\n",
      "Epoch 51: train loss=3.198, train acc=0.888\n",
      "\n",
      "Epoch 52: train loss=3.190, train acc=0.888\n",
      "\n",
      "Epoch 53: train loss=3.183, train acc=0.889\n",
      "\n",
      "Epoch 54: train loss=3.176, train acc=0.890\n",
      "\n",
      "Epoch 55: train loss=3.169, train acc=0.891\n",
      "\n",
      "Epoch 56: train loss=3.162, train acc=0.892\n",
      "\n",
      "Epoch 57: train loss=3.155, train acc=0.893\n",
      "\n",
      "Epoch 58: train loss=3.148, train acc=0.893\n",
      "\n",
      "Epoch 59: train loss=3.142, train acc=0.894\n",
      "\n",
      "Epoch 60: train loss=3.136, train acc=0.895\n",
      "\n",
      "Epoch 61: train loss=3.129, train acc=0.896\n",
      "\n",
      "Epoch 62: train loss=3.123, train acc=0.896\n",
      "\n",
      "Epoch 63: train loss=3.117, train acc=0.897\n",
      "\n",
      "Epoch 64: train loss=3.111, train acc=0.898\n",
      "\n",
      "Epoch 65: train loss=3.106, train acc=0.899\n",
      "\n",
      "Epoch 66: train loss=3.100, train acc=0.899\n",
      "\n",
      "Epoch 67: train loss=3.094, train acc=0.900\n",
      "\n",
      "Epoch 68: train loss=3.089, train acc=0.900\n",
      "\n",
      "Epoch 69: train loss=3.083, train acc=0.901\n",
      "\n",
      "Epoch 70: train loss=3.078, train acc=0.902\n",
      "\n",
      "Epoch 71: train loss=3.073, train acc=0.902\n",
      "\n",
      "Epoch 72: train loss=3.067, train acc=0.903\n",
      "\n",
      "Epoch 73: train loss=3.062, train acc=0.904\n",
      "\n",
      "Epoch 74: train loss=3.057, train acc=0.904\n",
      "\n",
      "Epoch 75: train loss=3.052, train acc=0.905\n",
      "\n",
      "Epoch 76: train loss=3.047, train acc=0.905\n",
      "\n",
      "Epoch 77: train loss=3.042, train acc=0.906\n",
      "\n",
      "Epoch 78: train loss=3.037, train acc=0.906\n",
      "\n",
      "Epoch 79: train loss=3.032, train acc=0.907\n",
      "\n",
      "Epoch 80: train loss=3.027, train acc=0.907\n",
      "\n",
      "Epoch 81: train loss=3.023, train acc=0.908\n",
      "\n",
      "Epoch 82: train loss=3.018, train acc=0.908\n",
      "\n",
      "Epoch 83: train loss=3.013, train acc=0.909\n",
      "\n",
      "Epoch 84: train loss=3.009, train acc=0.909\n",
      "\n",
      "Epoch 85: train loss=3.004, train acc=0.910\n",
      "\n",
      "Epoch 86: train loss=3.000, train acc=0.910\n",
      "\n",
      "Epoch 87: train loss=2.995, train acc=0.910\n",
      "\n",
      "Epoch 88: train loss=2.991, train acc=0.911\n",
      "\n",
      "Epoch 89: train loss=2.986, train acc=0.911\n",
      "\n",
      "Epoch 90: train loss=2.982, train acc=0.912\n",
      "\n",
      "Epoch 91: train loss=2.978, train acc=0.912\n",
      "\n",
      "Epoch 92: train loss=2.973, train acc=0.912\n",
      "\n",
      "Epoch 93: train loss=2.969, train acc=0.913\n",
      "\n",
      "Epoch 94: train loss=2.965, train acc=0.913\n",
      "\n",
      "Epoch 95: train loss=2.960, train acc=0.913\n",
      "\n",
      "Epoch 96: train loss=2.956, train acc=0.914\n",
      "\n",
      "Epoch 97: train loss=2.952, train acc=0.914\n",
      "\n",
      "Epoch 98: train loss=2.948, train acc=0.915\n",
      "\n",
      "Epoch 99: train loss=2.944, train acc=0.915\n",
      "\n",
      "Epoch 100: train loss=2.940, train acc=0.915\n",
      "\n",
      "Test: average loss=2.947, average accuracy=0.915\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "weight_decay_list = [0., 1e-4, 1e-2]\n",
    "FLAGS.learning_rate = 0.0001\n",
    "FLAGS.num_epoch = 100\n",
    "for weight_decay in weight_decay_list:\n",
    "    print('Weight decay of: {}'.format(weight_decay))\n",
    "    FLAGS.weight_decay = weight_decay \n",
    "    model = Dense(num_hidden=[256, 128, 10],\n",
    "                  weight_initializer=glorot_initializer,\n",
    "                  bias_initializer=zero_initializer,\n",
    "                  act=tf.nn.relu,\n",
    "                  logging=True)\n",
    "    with tf.Session() as sess:\n",
    "        log_file = str(model.act.__name__) + \"_\" + str(FLAGS.weight_decay)\n",
    "        train(x_train, y_train, x_validation, y_validation, x_test, y_test, model, sess, log_file)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 5\n",
    "By using visualiztion of learning curves (specially loss curve) try to explain the impact that L2 regularization had on training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note**: \n",
    "So far, some questions were placed between cells of codes and descriptions. \n",
    "\n",
    "In addition to **completing the code files**, please send a **report** including your answer to these questions as well. Do not forget to put the diagrams and visualizations needed in each part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
